{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def plot_outdata(dat1):\n",
        "    data1=dat1.detach().cpu()\n",
        "    # data2=dat2.detach().cpu()\n",
        "    rows,cols=data1.size()[0],4\n",
        "    j=1\n",
        "    num=data1.size()[0]\n",
        "\n",
        "    print(data1.dtype,data1.size())\n",
        "    for i in range(num):\n",
        "\n",
        "        plt.subplot(rows,cols,j)\n",
        "        plt.imshow(data1[i,0,:,:].squeeze(),cmap='gray')\n",
        "        plt.subplot(rows, cols, j+1)\n",
        "        plt.imshow(data1[i, 1, :, :].squeeze(), cmap='gray')\n",
        "        plt.subplot(rows, cols, j+2)\n",
        "        plt.imshow(data1[i, 2, :, :].squeeze(), cmap='gray')\n",
        "        plt.subplot(rows, cols, j+3)\n",
        "        plt.imshow(data1[i, 3, :, :].squeeze(), cmap='gray')\n",
        "        j=j+4\n",
        "\n",
        "    plt.show()\n",
        "# Two sequential convolution section of Unet\n",
        "def doubleconv(inp, out):\n",
        "    double_conv = nn.Sequential(\n",
        "        nn.Conv2d(inp, out, kernel_size=3,padding=1),\n",
        "        nn.BatchNorm2d(out,track_running_stats=False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out, out, kernel_size=3,padding=1),\n",
        "        nn.BatchNorm2d(out,track_running_stats=False),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return double_conv\n",
        "\n",
        "\n",
        "# Spatial Channel Attention Block\n",
        "class sca(Module):\n",
        "    def __init__(self, inp):\n",
        "        super(sca, self).__init__()\n",
        "        self.c_attn_conv = nn.Sequential(nn.Conv2d(inp, inp // 16, 1, bias=False),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Conv2d(inp // 16, inp, 1, bias=False)\n",
        "                                         )\n",
        "        self.c_sig = nn.Sigmoid()\n",
        "\n",
        "        self.avg_ch = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_ch = nn.AdaptiveMaxPool2d(1)\n",
        "        self.s_attn_conv = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7, padding=7 // 2, bias=False),\n",
        "                                         nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # Channel Attention\n",
        "        avg_ch_pool = self.avg_ch(input_tensor)\n",
        "        max_ch_pool = self.max_ch(input_tensor)\n",
        "\n",
        "        out_1 = self.c_attn_conv(avg_ch_pool)\n",
        "        out_2 = self.c_attn_conv(max_ch_pool)\n",
        "\n",
        "        c_sum = out_1+out_2\n",
        "        ch_out = self.c_sig(c_sum)\n",
        "        input_tensor = input_tensor * ch_out\n",
        "        # Spatial Attention\n",
        "        avg_pool = torch.mean(input_tensor, dim=1, keepdim=True)\n",
        "        max_pool = torch.max(input_tensor, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_pool, max_pool.values], dim=1)\n",
        "\n",
        "        x = self.s_attn_conv(x)\n",
        "        x = torch.mul(input_tensor, x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Function to apply spatial channel attention block\n",
        "def spatial_channel_attn(input_tensor):\n",
        "    input_tensor = input_tensor.type(torch.cuda.FloatTensor)\n",
        "    inp = input_tensor.size()[1]\n",
        "\n",
        "    sca_model = sca(inp).to(device)\n",
        "    x = sca_model(input_tensor)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Atrous spatial pyramid pooling block\n",
        "class aspp(Module):\n",
        "    def __init__(self, inp, out):\n",
        "        super(aspp, self).__init__()\n",
        "        self.aconv0 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=4, dilation=4, bias=False),\n",
        "                                    nn.BatchNorm2d(out, track_running_stats=False),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "        self.aconv1 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=6, dilation=6, bias=False),\n",
        "                                    nn.BatchNorm2d(out, track_running_stats=False),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "        self.aconv2 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=12, dilation=12, bias=False),\n",
        "                                    nn.BatchNorm2d(out, track_running_stats=False),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "        self.aconv3 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=1, dilation=1, bias=False),\n",
        "                                    nn.BatchNorm2d(out, track_running_stats=False),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "        self.aconv4 = nn.Sequential(nn.Conv2d(inp, out, kernel_size=3, stride=1, padding=2, dilation=2, bias=False),\n",
        "                                    nn.BatchNorm2d(out),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(out * 5, inp, kernel_size=1, stride=1, padding=0, dilation=1, bias=False),\n",
        "            nn.BatchNorm2d(inp),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        x0 = self.aconv0(input_tensor)\n",
        "        x1 = self.aconv1(input_tensor)\n",
        "        x2 = self.aconv2(input_tensor)\n",
        "        x3 = self.aconv3(input_tensor)\n",
        "        x4 = self.aconv4(input_tensor)\n",
        "        x = torch.cat((x0, x1, x2, x3, x4), dim=1)\n",
        "        aspp_out = self.final_conv(x)\n",
        "\n",
        "        return aspp_out\n",
        "\n",
        "# Function to carry out atrous spatial pyramid pooling (In our case input and output is made to be of same size)\n",
        "def atrous_spatial_pyramid_pooling(input_tensor):\n",
        "    input_tensor = input_tensor.type(torch.cuda.FloatTensor)\n",
        "    inp = input_tensor.size()[1]\n",
        "    out = inp // 4\n",
        "\n",
        "    asppmodel = aspp(inp, out).to(device)\n",
        "    aspp_out = asppmodel(input_tensor)\n",
        "    res_aspp = aspp_out+input_tensor\n",
        "    return res_aspp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Unet Model\n",
        "# Unet Model\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.maxpool2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.DownConv1 = doubleconv(1, 64)\n",
        "        self.DownConv2 = doubleconv(64, 128)\n",
        "        self.DownConv3 = doubleconv(128, 256)\n",
        "        self.DownConv4 = doubleconv(256, 512)\n",
        "        self.DownConv5 = doubleconv(512, 1024)\n",
        "\n",
        "        self.UpTrans1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.UpTrans2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.UpTrans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.UpTrans4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "\n",
        "        self.UpConv1 = doubleconv(1024, 512)\n",
        "        self.UpConv2 = doubleconv(512, 256)\n",
        "        self.UpConv3 = doubleconv(256, 128)\n",
        "        self.UpConv4 = doubleconv(128, 64)\n",
        "\n",
        "        self.out1 = nn.Conv2d(64, 3, kernel_size=1)\n",
        "        self.out2 = nn.Conv2d(128, 3, kernel_size=1)\n",
        "        self.out3 = nn.Conv2d(256, 3, kernel_size=1)\n",
        "        self.out4 = nn.Conv2d(512, 3, kernel_size=1)\n",
        "        self.soft=nn.Sigmoid()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.pre_classifier = nn.Sequential(nn.Conv2d(1024, 128, kernel_size=1), nn.ReLU())\n",
        "        self.classifier0 = nn.Sequential(nn.Linear(32768, 512),nn.ReLU())\n",
        "        self.classifier1 = nn.Sequential(nn.Linear(65536, 512),nn.ReLU())\n",
        "        self.classifier2 = nn.Sequential(nn.Linear(65536, 512),nn.ReLU())\n",
        "        self.classifier = nn.Sequential(nn.Linear(512 * 3, 3))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x=(batch size, channel, height, width)\n",
        "        # encoder\n",
        "\n",
        "        x1 = self.DownConv1(x)\n",
        "        x2 = self.maxpool2x2(x1)\n",
        "        x3 = self.DownConv2(x2)\n",
        "        x4 = self.maxpool2x2(x3)\n",
        "        x5 = self.DownConv3(x4)\n",
        "        x6 = self.maxpool2x2(x5)\n",
        "        x7 = self.DownConv4(x6)\n",
        "        x8 = self.maxpool2x2(x7)\n",
        "        x9 = self.DownConv5(x8)\n",
        "\n",
        "\n",
        "        # decoder\n",
        "        x9_aspp = atrous_spatial_pyramid_pooling(x9)\n",
        "        z1 = self.UpTrans1(x9_aspp)\n",
        "\n",
        "        x10 = self.UpConv1(spatial_channel_attn(torch.cat([x7, z1], 1)))\n",
        "\n",
        "        z2 = self.UpTrans2(x10)\n",
        "\n",
        "        x11 = self.UpConv2(spatial_channel_attn(torch.cat([x5, z2], 1)))\n",
        "\n",
        "        z3 = self.UpTrans3(x11)\n",
        "\n",
        "        x12 = self.UpConv3(spatial_channel_attn(torch.cat([x3, z3], 1)))\n",
        "\n",
        "        z4 = self.UpTrans4(x12)\n",
        "\n",
        "        x13 = self.UpConv4(spatial_channel_attn(torch.cat([x1, z4], 1)))\n",
        "\n",
        "        # ASPP\n",
        "        x13_aspp = atrous_spatial_pyramid_pooling(x13)\n",
        "        out1 = self.soft(self.out1(x13_aspp))\n",
        "        out2 = self.soft(self.out2(x12))\n",
        "        out3 = self.soft(self.out3(x11))\n",
        "        out4 = self.soft(self.out4(x10))\n",
        "\n",
        "\n",
        "        # classification branch\n",
        "\n",
        "        ###Classification#############\n",
        "        preclass_x=self.pre_classifier(x9_aspp)\n",
        "        x_flat=self.flatten(preclass_x)\n",
        "\n",
        "        out1_1 = self.classifier0(x_flat)\n",
        "        out2_1 = self.classifier1(self.flatten(out1[:, 0, :, :]))\n",
        "        out3_1 = self.classifier2(self.flatten(out1[:, 1, :, :]))\n",
        "\n",
        "        inp_1 = torch.concat([out1_1, out2_1, out3_1], dim=1)\n",
        "\n",
        "        output = self.classifier(inp_1)\n",
        "\n",
        "\n",
        "        return output,out1,out2,out3,out4\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}